{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from psaw import PushshiftAPI\n",
    "from nltk import ngrams\n",
    "from bs4 import *\n",
    "api = PushshiftAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # scrape reddit list\n",
    "# data = []\n",
    "# def next_page(soup):\n",
    "#     x = soup.find('ul',{'class':'pager'})\n",
    "#     s = str(x.select('a')[1])\n",
    "#     for i in re.findall(r'\"([^\"]*)\"', s):\n",
    "#         return(i)\n",
    "# link= \"/top-sfw-subreddits\"\n",
    "# i =1\n",
    "# while i <= 1000:\n",
    "    \n",
    "#     url = \"https://frontpagemetrics.com\" + link\n",
    "#     page = requests.get(url)\n",
    "\n",
    "#     soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "#     table = soup.find('table', {'class':\"table table-bordered\"})\n",
    "#     table_body = table.find('tbody')\n",
    "\n",
    "#     rows = table_body.find_all('tr')\n",
    "\n",
    "#     for row in rows:\n",
    "#         cols = row.find_all('td')\n",
    "#         cols = [ele.text.strip() for ele in cols]\n",
    "#         data.append([ele for ele in cols if ele])\n",
    "    \n",
    "#     link = next_page(soup)\n",
    "#     i+=1\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# subreddits = pd.DataFrame(data)\n",
    "# subreddits.columns=['rank','Subreddits', 'Subscribers']\n",
    "# subreddits.to_csv(\"list_of_subreddits.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that gets comments dataframe for a subreddit\n",
    "\n",
    "def get_pushshift_data(data_type, **kwargs):\n",
    "\n",
    " \n",
    "    base_url = f\"https://api.pushshift.io/reddit/search/{data_type}/\"\n",
    "    payload = kwargs\n",
    "    request = requests.get(base_url, params=payload)\n",
    "    data = request.json().get(\"data\")\n",
    "    df = pd.DataFrame.from_records(data)[['author','subreddit','body','created_utc']]\n",
    "    return df\n",
    "\n",
    "#  function that counts the number of n-grams in a column of a reddit dataframe\n",
    "\n",
    "def count_grams(df, i):\n",
    "    df_body = df['body']\n",
    "    \n",
    "    grams_dict = {}\n",
    "    for s in df_body:\n",
    "        grams = ngrams(s.split(),i)\n",
    "\n",
    "        for gram in grams:\n",
    "            gram2 = ' '.join(gram)\n",
    "            if gram2 in grams_dict:\n",
    "                grams_dict[gram2] += 1\n",
    "            else:\n",
    "                grams_dict[gram2] = 1\n",
    "\n",
    "    final_df = pd.DataFrame([grams_dict]).T\n",
    "\n",
    "    return final_df\n",
    "\n",
    "# collects gram counts from multiple subreddits\n",
    "def collect_grams(subs,i):\n",
    "    final_df = pd.DataFrame()\n",
    "    error_list = []\n",
    "    for s in subs:\n",
    "        try:\n",
    "            df1 = get_pushshift_data(data_type='comment',subreddit=s, after=\"30d\", size = 500)\n",
    "            count_df = count_grams(df1,i)\n",
    "            count_df[str(i)+'-gram'] = count_df.index\n",
    "            count_df['subreddit'] = s\n",
    "            count_df = count_df.rename(columns={0:'count'})\n",
    "            final_df = pd.concat([count_df, final_df], ignore_index=True)\n",
    "        except:\n",
    "            error_list.append(s)\n",
    "        \n",
    "    error = len(error_list)/len(subs)\n",
    "    print(error)\n",
    "    \n",
    "    return final_df, error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_df = pd.read_csv('list_of_subreddits.csv')\n",
    "subs_list = [s[3:] for s in subs_df['Subreddits']]\n",
    "\n",
    "subs_list\n",
    "subs_100 = subs_list[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(       count      1-gram            subreddit\n",
       " 56127    266          to                Music\n",
       " 35102    265         the           philosophy\n",
       " 56097    260         the                Music\n",
       " 16664    206         the               bestof\n",
       " 56214    189         you                Music\n",
       " 27730    180         the        AdviceAnimals\n",
       " 9858     173          to  relationship_advice\n",
       " 52910    172         the            worldnews\n",
       " 35150    170          to           philosophy\n",
       " 44412    170         the                books\n",
       " 33766    160           a           philosophy\n",
       " 34693    159          of           philosophy\n",
       " 39086    157          to        Documentaries\n",
       " 55408    155         and                Music\n",
       " 33834    151         and           philosophy\n",
       " 56217    148        your                Music\n",
       " 16714    143          to               bestof\n",
       " 55858    142          of                Music\n",
       " 43269    140           a                books\n",
       " 48956    139         the               movies\n",
       " 39041    139         the        Documentaries\n",
       " 1411     138         the              pokemon\n",
       " 33622    136           I           philosophy\n",
       " 43130    136           I                books\n",
       " 55367    135           a                Music\n",
       " 34461    135          is           philosophy\n",
       " 20306    134         the               travel\n",
       " 38088    133           a        Documentaries\n",
       " 38760    132          of        Documentaries\n",
       " 31556    131         the      personalfinance\n",
       " ...      ...         ...                  ...\n",
       " 22882      1       exact    interestingasfuck\n",
       " 22883      1    exactly?    interestingasfuck\n",
       " 22885      1  experiment    interestingasfuck\n",
       " 22886      1     expired    interestingasfuck\n",
       " 22888      1     extinct    interestingasfuck\n",
       " 22860      1     don't),    interestingasfuck\n",
       " 22857      1      doing.    interestingasfuck\n",
       " 22856      1     doesnâ€™t    interestingasfuck\n",
       " 22840      1    diameter    interestingasfuck\n",
       " 22829      1        date    interestingasfuck\n",
       " 22830      1       dates    interestingasfuck\n",
       " 22831      1        day.    interestingasfuck\n",
       " 22832      1     dealers    interestingasfuck\n",
       " 22833      1        debt    interestingasfuck\n",
       " 22834      1      decide    interestingasfuck\n",
       " 22836      1   delicious    interestingasfuck\n",
       " 22838      1    deserves    interestingasfuck\n",
       " 22839      1     destroy    interestingasfuck\n",
       " 22841      1        dick    interestingasfuck\n",
       " 22853      1      doctor    interestingasfuck\n",
       " 22842      1         did    interestingasfuck\n",
       " 22844      1        diet    interestingasfuck\n",
       " 22845      1  difference    interestingasfuck\n",
       " 22846      1   difficult    interestingasfuck\n",
       " 22847      1    diluting    interestingasfuck\n",
       " 22848      1      dipped    interestingasfuck\n",
       " 22849      1   direction    interestingasfuck\n",
       " 22850      1     discuss    interestingasfuck\n",
       " 22851      1      divide    interestingasfuck\n",
       " 59889      1           ðŸ˜‘                funny\n",
       " \n",
       " [59890 rows x 3 columns],\n",
       " ['announcements',\n",
       "  'news',\n",
       "  'EarthPorn',\n",
       "  'IAmA',\n",
       "  'gifs',\n",
       "  'askscience',\n",
       "  'Jokes',\n",
       "  'explainlikeimfive',\n",
       "  'Art',\n",
       "  'nottheonion',\n",
       "  'DIY',\n",
       "  'gadgets',\n",
       "  'photoshopbattles',\n",
       "  'GetMotivated',\n",
       "  'UpliftingNews',\n",
       "  'listentothis',\n",
       "  'InternetIsBeautiful',\n",
       "  'history',\n",
       "  'dataisbeautiful',\n",
       "  'Futurology',\n",
       "  'WritingPrompts',\n",
       "  'memes',\n",
       "  'pcmasterrace',\n",
       "  'Whatcouldgowrong',\n",
       "  'dankmemes',\n",
       "  'Tinder',\n",
       "  'PS4',\n",
       "  'BikiniBottomTwitter',\n",
       "  'AnimalsBeingBros',\n",
       "  'photography',\n",
       "  'dadjokes',\n",
       "  'AnimalsBeingJerks',\n",
       "  'nextfuckinglevel',\n",
       "  'buildapc',\n",
       "  'woahdude',\n",
       "  'Unexpected',\n",
       "  'PewdiepieSubmissions',\n",
       "  'gardening',\n",
       "  'boardgames',\n",
       "  'WatchPeopleDieInside',\n",
       "  'ContagiousLaughter',\n",
       "  'programming',\n",
       "  'Parenting',\n",
       "  'mildlyinfuriating'])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect_grams(subs_100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Empty DataFrame\n",
       " Columns: []\n",
       " Index: [], ['announcements'])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect_grams(['announcements'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
